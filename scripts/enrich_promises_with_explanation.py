#!/usr/bin/env python
# PromiseTracker/scripts/enrich_promises_with_explanation.py
# This script enriches promises with explanations generated by an LLM.

import firebase_admin
from firebase_admin import firestore, credentials
import os
from google import genai
import time
import asyncio
import logging
import traceback
from dotenv import load_dotenv
import json
import argparse
from datetime import datetime, timezone
import re
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
import httpx # Required for the exception type
from google.api_core import exceptions as google_api_core_exceptions # Import Google API core exceptions

# Attempt to import from sibling directory common_utils
try:
    import common_utils 
except ImportError:
    logging.warning("Could not import common_utils. Department standardization will not be available if common_utils is missing.")
    common_utils = None # Ensure it's defined even if import fails

# --- Load Environment Variables ---
load_dotenv()
# --- End Load Environment Variables ---

# --- Logger Setup ---
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(filename)s:%(lineno)d - %(message)s',
                    datefmt='%Y-%m-%d %H:%M:%S')
logger = logging.getLogger("enrich_promises_explanation")
# --- End Logger Setup ---

# --- Firebase Configuration ---
db = None
if not firebase_admin._apps:
    try:
        firebase_admin.initialize_app()
        project_id = os.getenv('FIREBASE_PROJECT_ID', '[Not Set - Using Default]')
        logger.info(f"Connected to CLOUD Firestore (Project: {project_id}) using default credentials.")
        db = firestore.client()
    except Exception as e_default:
        logger.warning(f"Cloud Firestore init with default creds failed: {e_default}. Attempting service account.")
        cred_path = os.getenv('FIREBASE_SERVICE_ACCOUNT_KEY_PATH')
        if cred_path:
            try:
                logger.info(f"Attempting Firebase init with service account key from env var: {cred_path}")
                cred = credentials.Certificate(cred_path)
                app_name = 'enrich_promises_app'
                try:
                    firebase_admin.initialize_app(cred, name=app_name)
                except ValueError: # App already exists
                    app_name_unique = f"{app_name}_{str(time.time())}"
                    firebase_admin.initialize_app(cred, name=app_name_unique)
                    app_name = app_name_unique

                project_id_sa = os.getenv('FIREBASE_PROJECT_ID', '[Not Set - Using Service Account]')
                logger.info(f"Connected to CLOUD Firestore (Project: {project_id_sa}) via service account.")
                db = firestore.client(app=firebase_admin.get_app(name=app_name))
            except Exception as e_sa:
                logger.critical(f"Firebase init with service account key from {cred_path} failed: {e_sa}", exc_info=True)
        else:
            logger.warning("FIREBASE_SERVICE_ACCOUNT_KEY_PATH environment variable not set, and default creds failed.")

if db is None:
    logger.critical("CRITICAL: Failed to obtain Firestore client. Exiting.")
    exit("Exiting: Firestore client not available.")
# --- End Firebase Configuration ---

# --- Gemini Configuration ---
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
if not GEMINI_API_KEY:
    logger.critical("GEMINI_API_KEY not found in environment variables or .env file.")
    exit("Exiting: Missing GEMINI_API_KEY.")

if "GOOGLE_API_KEY" not in os.environ and GEMINI_API_KEY:
    os.environ["GOOGLE_API_KEY"] = GEMINI_API_KEY

LLM_MODEL_NAME = os.getenv("GEMINI_MODEL_NAME_EXPLANATION_ENRICHMENT", "models/gemini-2.5-flash-preview-05-20") 
GENERATION_CONFIG_DICT = {
    "temperature": 0.7, # Slightly higher for more creative text generation, but still factual
    "top_p": 0.95,
    "top_k": 64,
    "max_output_tokens": 65536, # Adjusted based on typical output needs for 3 fields * N promises
    "response_mime_type": "application/json",
}

client = None
try:
    client = genai.Client() 
    logger.info(f"Successfully initialized Gemini Client with model {LLM_MODEL_NAME}.")
except Exception as e:
    logger.critical(f"Failed to initialize Gemini client: {e}", exc_info=True)
    exit("Exiting: Gemini client initialization failed.")
# --- End Gemini Configuration ---

# --- Constants ---
PROMISES_COLLECTION_ROOT = os.getenv("TARGET_PROMISES_COLLECTION", "promises")
# Define the target source_types
TARGET_SOURCE_TYPES = ["2021 LPC Mandate Letters", "2025 LPC Platform"]


SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PROMPT_FILE_PATH = os.path.join(SCRIPT_DIR, "..", "prompts", "prompt_generate_whatitmeans.md")

DEFAULT_REGION_CODE = "Canada"
KNOWN_PARTY_CODES = ["LPC"] # Focusing on LPC as per source_type
RATE_LIMIT_DELAY_SECONDS = 5 
# --- End Constants ---


def clean_json_from_markdown(text_blob: str) -> str:
    """
    Extracts a JSON string from a potential markdown code block.
    Handles ```json ... ``` or ``` ... ```.
    """
    logger.debug(f"clean_json_from_markdown received (repr, first 100 chars): {repr(text_blob[:100])}")
    regex_pattern = r"```(?:json)?\s*([\s\S]+?)\s*```"
    match = re.search(regex_pattern, text_blob)
    if match:
        extracted_json = match.group(1).strip()
        logger.debug(f"clean_json_from_markdown: Regex matched. Extracted JSON (repr, first 100 chars): {repr(extracted_json[:100])}")
        return extracted_json
    else:
        # If no markdown block, assume the text_blob is already the JSON or needs stripping
        logger.debug(f"clean_json_from_markdown: Regex did NOT match. Pattern was: {regex_pattern}. Returning stripped original (repr, first 100 chars): {repr(text_blob.strip()[:100])}")
        return text_blob.strip()


async def query_promises_for_enrichment(parliament_session_id: str, limit: int | None, force_reprocessing: bool) -> list[dict]:
    """
    Queries promises from Firestore for LPC party based on:
    - parliament_session_id
    - source_type in TARGET_SOURCE_TYPES
    - Optionally, only those not yet processed for these fields (unless force_reprocessing).
    """
    logger.info(f"Querying promises for enrichment: session '{parliament_session_id}', limit: {limit}, force: {force_reprocessing}")
    all_matching_promises = []
    
    # Hardcoded to LPC as per the source types
    party_code = "LPC" 
    logger.debug(f"Querying flat promises collection for party: {party_code}")

    try:
        query = db.collection(PROMISES_COLLECTION_ROOT).where(
            filter=firestore.FieldFilter("party_code", "==", party_code)
        ).where(
            filter=firestore.FieldFilter("parliament_session_id", "==", parliament_session_id)
        ).where(
            filter=firestore.FieldFilter("source_type", "in", TARGET_SOURCE_TYPES)
        )

        if not force_reprocessing:
            # This logic aims to skip already processed items.
            # A single field check might be enough, or a dedicated status field.
            # Using 'what_it_means_for_canadians' as a proxy for completion of this script's task.
             query = query.where(
                 filter=firestore.FieldFilter("what_it_means_for_canadians", "==", None)
             )
            # Note: Firestore cannot do "OR" for "is null" on multiple fields directly.
            # If you need to check if *any* of the three fields are missing, you might need to fetch more broadly
            # and filter client-side, or add a specific "explanation_enriched_vX" status field.
            # For now, checking one field as a proxy.

        if limit:
            query = query.limit(limit)

        promise_docs_stream = query.select([
            "text", "responsible_department_lead", "source_type" # Add other fields if needed by LLM or logic
        ]).stream() # Fetching only necessary fields

        for doc_snapshot in await asyncio.to_thread(list, promise_docs_stream):
            promise_data = doc_snapshot.to_dict()
            if promise_data and promise_data.get("text"):
                all_matching_promises.append({
                    "id": doc_snapshot.id,
                    "text": promise_data["text"],
                    "responsible_department_lead": promise_data.get("responsible_department_lead"),
                    "source_type": promise_data.get("source_type"),
                    "doc_ref": doc_snapshot.reference # Store the document reference
                })
            else:
                logger.warning(f"Promise {doc_snapshot.id} for party {party_code} missing 'text' field, skipping.")
        
        logger.info(f"Retrieved {len(all_matching_promises)} promises for enrichment based on criteria.")
        return all_matching_promises
    except Exception as e:
        logger.error(f"Error querying promises for party {party_code} in flat collection: {e}", exc_info=True)
        return []


async def load_llm_prompt(prompt_file: str) -> str:
    """Loads the LLM prompt from a specified file."""
    try:
        with open(prompt_file, 'r') as f:
            prompt_content = f.read()
        logger.info(f"Successfully loaded LLM prompt from {prompt_file}")
        return prompt_content
    except FileNotFoundError:
        logger.critical(f"Prompt file not found: {prompt_file}. Please ensure it exists.")
        exit(f"Exiting: Prompt file {prompt_file} not found.")
    except Exception as e:
        logger.critical(f"Error reading prompt file {prompt_file}: {e}", exc_info=True)
        exit(f"Exiting: Could not read prompt file {prompt_file}.")


@retry(
    wait=wait_exponential(multiplier=1, min=4, max=60),  # Exponential backoff: 1s, 2s, 4s, 8s ... up to 60s
    stop=stop_after_attempt(5),  # Retry up to 5 times
    retry=retry_if_exception_type((
        httpx.ConnectError, 
        httpx.ReadTimeout, 
        httpx.NetworkError, 
        google_api_core_exceptions.InternalServerError,  # Corrected exception type
        google_api_core_exceptions.ServiceUnavailable  # Added for 503 errors
    )), 
    before_sleep=lambda retry_state: logger.warning(f"Retrying LLM call due to {retry_state.outcome.exception()}, attempt {retry_state.attempt_number} after waiting {retry_state.next_action.sleep}s...")
)
async def generate_explanations_with_llm(promises_data: list[dict], base_prompt: str) -> list[dict] | None:
    """ 
    Generates explanations for a list of promises using the LLM.
    
    Args:
        promises_data: A list of dictionaries, each containing at least 'text'.
        base_prompt: The base LLM prompt template.

    Returns:
        A list of dictionaries structured according to the LLM output if successful, otherwise None.
    """
    if not promises_data:
        logger.info("No promises provided to LLM for explanation generation.")
        return []

    # Construct the part of the prompt that lists the commitments.
    # This assumes base_prompt does NOT contain this header, and the script appends it.
    commitments_section_header = "\n\n**Commitments to Process:**\n"
    commitments_list_str = ""
    for i, promise in enumerate(promises_data):
        commitments_list_str += f"* {promise['text']}\n" 
    
    full_prompt = base_prompt + commitments_section_header + commitments_list_str
    
    logger.debug(f"Constructed full LLM prompt. Length: {len(full_prompt)}")
    logger.debug(f"Full prompt starts with: {full_prompt[:200]}...") # Avoid logging full sensitive prompt in prod

    try:
        logger.info(f"Sending {len(promises_data)} promises to LLM ({LLM_MODEL_NAME}) for explanation generation.")
        
        if not client:
            logger.critical("Gemini client not initialized. Cannot call LLM.")
            return None
        
        # Corrected API call based on google-generativeai library structure
        response = await client.aio.models.generate_content(
            model=LLM_MODEL_NAME, 
            contents=[full_prompt] # contents should be a list
            # Relying on the GENERATION_CONFIG_DICT (especially response_mime_type) being respected by the client/model implicitly.
            # temperature=GENERATION_CONFIG_DICT["temperature"],
            # top_p=GENERATION_CONFIG_DICT["top_p"],
            # top_k=GENERATION_CONFIG_DICT["top_k"],
            # max_output_tokens=GENERATION_CONFIG_DICT["max_output_tokens"],
            # response_mime_type=GENERATION_CONFIG_DICT["response_mime_type"]
            # tools=TOOLS_LIST # Only if specific tools are registered and needed
        )
        
        raw_response_text = response.text
        logger.debug(f"LLM Raw Response Text (first 500 chars): {raw_response_text[:500]}")

        cleaned_response_text = clean_json_from_markdown(raw_response_text)
        logger.debug(f"LLM Cleaned Response Text (first 500 chars): {cleaned_response_text[:500]}")
        
        llm_output = json.loads(cleaned_response_text)

        if not isinstance(llm_output, list):
            logger.error(f"LLM output is not a list as expected. Type: {type(llm_output)}. Response: {cleaned_response_text[:1000]}")
            return None
        
        if len(llm_output) != len(promises_data):
            logger.warning(f"LLM output items ({len(llm_output)}) does not match input promises ({len(promises_data)}). This might indicate partial processing or an issue. Will attempt to match by 'commitment_text'.")

        validated_output = []
        for i, item in enumerate(llm_output):
            if not isinstance(item, dict) or \
               "commitment_text" not in item or \
               "concise_title" not in item or \
               "what_it_means_for_canadians" not in item or \
               "description" not in item or \
               "background_and_context" not in item:
                logger.warning(f"LLM output item {i} has incorrect structure or missing key fields (e.g., concise_title, description): {str(item)[:200]}. Skipping this item.")
                continue 
            validated_output.append(item)
        
        if not validated_output and llm_output:
             logger.error("All LLM output items were malformed, though a list was returned.")
             return None

        logger.info(f"Successfully received and parsed LLM response. Found explanations for {len(validated_output)} commitments.")
        return validated_output

    except json.JSONDecodeError as json_err:
        logger.error(f"LLM response was not valid JSON. Error: {json_err}. Raw Response (after cleaning attempt): \n{cleaned_response_text if 'cleaned_response_text' in locals() else raw_response_text if 'raw_response_text' in locals() else 'Response not captured'}", exc_info=True)
        return None
    except Exception as e:
        if hasattr(e, 'message') and "response was blocked" in str(e.message).lower(): # Check if e.message is not None
             logger.error(f"LLM generation failed because the response was blocked. Details: {e}", exc_info=True)
        elif hasattr(e, 'message') and "API key not valid" in str(e.message): 
             logger.error(f"LLM generation failed due to an invalid API key. Details: {e}", exc_info=True)    
        else:
            logger.error(f"Error calling LLM for explanation generation: {e}", exc_info=True)
        return None


async def store_explanations(
    llm_results: list[dict],
    original_promises_data: list[dict], # To map results back to Firestore doc_refs
    dry_run: bool,
    db_batch_size: int = 100 # Smaller batch for updates
) -> int:
    """
    Stores the LLM-generated explanations back to the corresponding promise documents in Firestore.
    Matches LLM results to original promises using 'commitment_text'.
    Returns the count of successfully updated promises.
    """
    if not llm_results:
        logger.info("No LLM results to store.")
        return 0

    updated_promise_count = 0
    firestore_batch = db.batch()
    operations_in_batch = 0

    # Create a map from original promise text to its doc_ref for efficient updates
    promise_text_to_ref_map = {p['text']: p['doc_ref'] for p in original_promises_data}

    for llm_item in llm_results:
        commitment_text_from_llm = llm_item.get("commitment_text")
        target_promise_ref = promise_text_to_ref_map.get(commitment_text_from_llm)

        if not target_promise_ref:
            logger.warning(f"Could not match LLM commitment text to an original promise for updating. Text: {commitment_text_from_llm[:100]}... Skipping this result.")
            continue

        logger.info(f"Processed promise ID: {target_promise_ref.id} for update.")

        payload = {
            "concise_title": llm_item.get("concise_title"),
            "what_it_means_for_canadians": llm_item.get("what_it_means_for_canadians"),
            "description": llm_item.get("description"),
            "background_and_context": llm_item.get("background_and_context"),
            "dev_explanation_enriched_at": firestore.SERVER_TIMESTAMP,
            "dev_explanation_enrichment_model": LLM_MODEL_NAME,
            "dev_explanation_enrichment_status": "processed"
        }

        if not dry_run:
            firestore_batch.update(target_promise_ref, payload)
            operations_in_batch += 1
            logger.debug(f"Scheduled update for promise (text: {commitment_text_from_llm[:50]}...)")
        else:
            logger.info(f"[DRY RUN] Would update promise (text: {commitment_text_from_llm[:50]}...) with payload: {payload}")

        updated_promise_count += 1 # Count successful mapping, actual write happens in batch

        if operations_in_batch >= db_batch_size and not dry_run:
            logger.info(f"Committing batch of {operations_in_batch} Firestore updates...")
            try:
                await asyncio.to_thread(firestore_batch.commit)
                logger.info("Batch committed.")
            except Exception as e_commit:
                logger.error(f"Error committing Firestore batch: {e_commit}", exc_info=True)
                # Decide how to handle batch commit errors (e.g., retry, log and skip)
            firestore_batch = db.batch() # Start a new batch
            operations_in_batch = 0
            
    # Commit any remaining operations in the last batch
    if operations_in_batch > 0 and not dry_run:
        logger.info(f"Committing final batch of {operations_in_batch} Firestore updates...")
        try:
            await asyncio.to_thread(firestore_batch.commit)
            logger.info("Final batch committed.")
        except Exception as e_commit_final:
            logger.error(f"Error committing final Firestore batch: {e_commit_final}", exc_info=True)
    elif operations_in_batch > 0 and dry_run:
        logger.info(f"[DRY RUN] Would commit final batch of {operations_in_batch} Firestore operations.")

    logger.info(f"Finished storing explanations. Updated {updated_promise_count} promises (pending batch commit for non-dry run).")
    return updated_promise_count


async def main_async_entrypoint():
    parser = argparse.ArgumentParser(description='Enrich promises with LLM-generated explanations.')
    parser.add_argument(
        '--parliament_session_id',
        type=str,
        required=True,
        help='The parliament_session_id to filter promises (e.g., "44").'
    )
    parser.add_argument(
        '--limit_promises',
        type=int,
        default=None, # Process all matching promises by default
        help='Optional limit on the number of promises to process in one run.'
    )
    parser.add_argument(
        '--force_reprocessing',
        action='store_true',
        help='Force reprocessing of explanations even if they seem to exist.'
    )
    parser.add_argument(
        '--dry_run',
        action='store_true',
        help='Perform a dry run without making any changes to Firestore.'
    )
    parser.add_argument(
        '--batch_size_llm',
        type=int,
        default=5, # Number of promises to send to LLM in one go
        help='Number of promises to batch together for a single LLM call.'
    )

    args = parser.parse_args()

    logger.info("--- Starting Promise Explanation Enrichment Script ---")
    if args.dry_run:
        logger.info("*** DRY RUN MODE ENABLED: No changes will be written to Firestore. ***")
    logger.info(f"Config: Parliament Session ID: {args.parliament_session_id}, Limit: {args.limit_promises}, Force Reprocessing: {args.force_reprocessing}, LLM Batch Size: {args.batch_size_llm}")

    # 1. Load LLM Prompt Template
    base_prompt_template = await load_llm_prompt(PROMPT_FILE_PATH)
    
    # 2. Query Promises from Firestore
    # The limit here is for the total number of promises to process in the script run.
    # The actual fetching might be more if force_reprocessing is false and many are already done.
    promises_to_process_initially = await query_promises_for_enrichment(
        parliament_session_id=args.parliament_session_id,
        limit=args.limit_promises, # This limit is applied in the query if not forcing
        force_reprocessing=args.force_reprocessing
    )

    if not promises_to_process_initially:
        logger.info("No promises found matching the criteria for enrichment. Exiting.")
        return

    logger.info(f"Found {len(promises_to_process_initially)} promises requiring explanation enrichment.")
    
    # Apply overall limit if one was provided, after fetching (especially if not force_reprocessing, where query limit might not apply)
    if args.limit_promises is not None:
        promises_to_process_final = promises_to_process_initially[:args.limit_promises]
        if len(promises_to_process_final) < len(promises_to_process_initially):
            logger.info(f"Applied overall limit: processing {len(promises_to_process_final)} out of {len(promises_to_process_initially)} found.")
    else:
        promises_to_process_final = promises_to_process_initially


    total_successfully_enriched_count = 0

    # 3. Process in batches for LLM
    for i in range(0, len(promises_to_process_final), args.batch_size_llm):
        batch_of_promises = promises_to_process_final[i:i + args.batch_size_llm]
        logger.info(f"Processing batch {i // args.batch_size_llm + 1}/{(len(promises_to_process_final) + args.batch_size_llm -1)//args.batch_size_llm} (Size: {len(batch_of_promises)})")

        if not batch_of_promises:
            continue

        llm_generated_explanations = await generate_explanations_with_llm(batch_of_promises, base_prompt_template)

        if llm_generated_explanations:
            logger.info(f"LLM returned {len(llm_generated_explanations)} explanation sets for the batch.")
            # 4. Store explanations back to Firestore
            updated_count_for_batch = await store_explanations(
                llm_generated_explanations,
                batch_of_promises, # Pass the current batch for doc_ref mapping
                args.dry_run
            )
            total_successfully_enriched_count += updated_count_for_batch
            logger.info(f"Updated {updated_count_for_batch} promises in this batch.")
        else:
            logger.warning(f"LLM did not return valid explanations for batch starting at index {i}. Skipping storage for this batch.")
            # Log which promises were in this failed batch for investigation
            for p_fail in batch_of_promises:
                logger.warning(f"  - Failed enrichment for promise ID: {p_fail['id']}, Text: {p_fail['text'][:70]}...")
                if not args.dry_run: # If not dry run, mark as failed
                    try:
                        await asyncio.to_thread(
                            p_fail['doc_ref'].update, {
                                "dev_explanation_enrichment_status": "failed_llm_generation",
                                "dev_explanation_enriched_at": firestore.SERVER_TIMESTAMP,
                            }
                        )
                    except Exception as e_mark_fail:
                        logger.error(f"Error marking promise {p_fail['id']} as failed: {e_mark_fail}")


        if i + args.batch_size_llm < len(promises_to_process_final):
            logger.info(f"Waiting for {RATE_LIMIT_DELAY_SECONDS}s before next LLM batch to manage rate limits...")
            await asyncio.sleep(RATE_LIMIT_DELAY_SECONDS)

    logger.info(f"--- Promise Explanation Enrichment Script Finished ---")
    logger.info(f"Total promises successfully enriched in this run: {total_successfully_enriched_count}")

if __name__ == "__main__":
    asyncio.run(main_async_entrypoint()) 